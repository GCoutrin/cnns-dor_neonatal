{"cells":[{"cell_type":"markdown","source":["# Introdução"],"metadata":{"id":"OTwiVdVE9yu2"}},{"cell_type":"markdown","source":["Este notebook apresenta a rotina de treinamento e validação aplicada à 5 CNNs distintas, considerando a tarefa do reconhecimento de expressões facias de dor em recém-nascidos.\n","\n","As imagens consumidas por este script foram antes processadas pela rotina apresentada no arquivo Prepara_imagens.ipynb\n","\n","As métricas de desempenho obtidas com a execução deste notebook foram, posteriormente, processadas pelo arquivo ANOVA.ipynb\n","\n","Após a execução do presente notebook, os modelos resultantes foram submetidos ao método Grad-CAM por meio do script apresentado em GradCAM.ipynb  "],"metadata":{"id":"Ey-O3_we911t"}},{"cell_type":"markdown","metadata":{"gradient":{"editing":false,"id":"d4bfad25-af58-4290-8b82-baeb9c0205b0","kernelId":"0e37b7b3-4d80-4a5d-9bda-95cf45add740"},"id":"ZxfZrl8JDHFn"},"source":["# Configuração Inicial"]},{"cell_type":"markdown","source":["Modelo a ser treinado\n","\n","Opções:\n","\n","\n","* vgg16: VGG-16 pré-treinada no banco de imagens de face VGGFace \n","* resnet50: ResNet50 pré-treinada no banco de imagens de face VGGFace2 \n","* senet50: SENet50 pré-treinada no banco de imagens de face VGGFace2 \n","* incepv3: Inception-V3 pré-treinada no base ImageNet\n","* ncnn: Neonatal Convolutional Neural Network (N-CNN) sem treinamento prévio \n","\n"],"metadata":{"id":"bB3OrGkKwmQl"}},{"cell_type":"code","source":["## SELECIONE UM MODEL_NAME ##\n","\n","model_name='vgg16'\n","#model_name='resnet50'\n","#model_name='senet50'\n","#model_name='incepv3'\n","#model_name='ncnn'"],"metadata":{"id":"TJsFRqD0xM_E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bibliotecas"],"metadata":{"id":"OtquUMuytfzu"}},{"cell_type":"code","source":["import os\n","import os.path as osp\n","import sys\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import pandas as pd\n","from shutil import copyfile, copytree\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, roc_curve, f1_score, roc_auc_score,confusion_matrix, recall_score, precision_score\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n","from tensorflow.keras.regularizers import l1\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras import layers"],"metadata":{"id":"TGjLh7xhsjOF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hiperparâmetros"],"metadata":{"id":"WusKaMUgti5S"}},{"cell_type":"code","source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","BATCH_SIZE = 16\n","LEARNING_RATE = 1e-4\n","LEARNING_RATE_ft = 1e-6    # _ft = fine-tuning\n","CHANNELS = 3 \n","REGULARIZER = l1(5e-4)\n","BUFFER_SIZE = 2048\n","EPOCHS = 1000\n","OPTIMIZER = RMSprop(learning_rate=LEARNING_RATE)\n","OPTIMIZER_ft = RMSprop(learning_rate=LEARNING_RATE_ft)  # _ft = fine-tuning\n","LOSS = tf.keras.losses.SparseCategoricalCrossentropy() \n","\n","if model_name == \"incepv3\":\n","\n","  IMG_HEIGHT = 299 \n","  IMG_WIDTH = 299\n","\n","elif model_name == \"ncnn\":\n","\n","  IMG_HEIGHT = 120\n","  IMG_WIDTH = 120\n","\n","else:\n","  IMG_HEIGHT = 224\n","  IMG_WIDTH = 224 "],"metadata":{"id":"_F6NI-gJsrCF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Biblioteca dos modelos pré-treinados em imagens de face: VGG-16, ResNet50 e SENet50"],"metadata":{"id":"BloFbUHTtmcO"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T22:56:44.165197Z","iopub.status.busy":"2022-10-05T22:56:44.164716Z","iopub.status.idle":"2022-10-05T22:57:17.115595Z","shell.execute_reply":"2022-10-05T22:57:17.114156Z","shell.execute_reply.started":"2022-10-05T22:56:44.165102Z"},"gradient":{"editing":false,"id":"69adc95a-cdbd-4ebd-9185-eba8866ad0ce","kernelId":"0e37b7b3-4d80-4a5d-9bda-95cf45add740"},"id":"tp6ucqHVCyZA"},"outputs":[],"source":["#Instalação da biblioteca\n","!pip install git+https://github.com/rcmalli/keras-vggface.git --quiet\n","!pip install keras_applications --no-deps\n","\n","#ajuste de dependências do arquivo \"models.py\"\n","filename = \"/usr/local/lib/python\" + sys.version[0:3] + \"/dist-packages/keras_vggface/models.py\"\n","text = open(filename).read()\n","open(filename, \"w+\").write(text.replace('keras.engine.topology', 'tensorflow.keras.utils'))\n","\n","#importando biblioteca instalada\n","from keras_vggface.vggface import VGGFace\n","from keras_vggface import utils\n"]},{"cell_type":"markdown","source":["Função que instancia modelos pré-treinados"],"metadata":{"id":"IRRmSA9arAq5"}},{"cell_type":"code","source":["# definição de função para instanciar modelo pré-treinado\n","def base_model(model_name):\n","  \"\"\"\n","  **valores de model_name**\n","\n","  vgg16: VGG-16 pré-treinada no banco de imagens de face VGGFace (modelo disponibilizado por R.C. Malli)\n","  resnet50: ResNet50 pré-treinada no banco de imagens de face VGGFace2 (modelo disponibilizado por R.C. Malli)\n","  senet50: SENet50 pré-treinada no banco de imagens de face VGGFace2 (modelo disponibilizado por R.C. Malli)\n","  incepV3: Inception-V3 pré-treinada no base ImageNet (modelo disponibilizado pelo TensorFlow)\n","\n","  \"\"\"\n","  if model_name == \"vgg16\":\n","    PRE_TRAINED = VGGFace(include_top=False, input_shape=(224,224,3), model= model_name, pooling=None)\n","\n","  elif model_name in ['resnet50','senet50']:\n","    PRE_TRAINED = VGGFace(include_top=False, input_shape=(224,224,3), model= model_name, pooling='avg')\n","\n","  elif model_name == \"incepv3\":\n","    PRE_TRAINED = tf.keras.applications.inception_v3.InceptionV3(\n","    include_top=False, weights='imagenet', input_tensor=None,\n","    input_shape=input_size, pooling='avg')\n","\n","  return PRE_TRAINED"],"metadata":{"id":"kmvkmkywq8Kg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implementação da N-CNN no TensorFlow"],"metadata":{"id":"H4x4oJPXpnYm"}},{"cell_type":"markdown","source":["A Neonatal Convolutional Neural Network (N-CNN) é a primeira CNN desenvolvida especificamente para o reconhecimento da dor neonatal em imagens de face. O modelo foi proposto por Zamzmi et al. (2019) em \"Pain assessment from facial expression: Neonatal convolutional neural\n","network (N-CNN)\", que está disponível em https://ieeexplore.ieee.org/abstract/document/8851879\n","\n","Até o momento do desenvolvimento do presente trabalho, não há versão do modelo disponível *online*, portanto, realizou-se aqui sua implementação no TensorFlow, seguindo a descrição apresentada em Zamzmi et al. (2019). No entanto, a última cada do modelo foi alterada: de uma saída sigmoide com um neurônio, passou a ser uma saída softmax com 2 neurônios. Este ajuste foi feito para a aplicação do Grad-CAM, encontrada no arquivo GradCAM.ipynb"],"metadata":{"id":"arpeu3yP-4z-"}},{"cell_type":"code","source":["# função que instancia uma N-CNN\n","def NCNN():\n","  \n","  input = tf.keras.layers.Input(shape = (120,120,3), name='input')\n","\n","  # Branch 1\n","  maxpool_1 = tf.keras.layers.MaxPooling2D(pool_size=10, strides=10,\n","                                          padding='same', name='maxpool_10x10') (input)\n","\n","  # Branch 2\n","  conv_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=(1,1),\n","                                  padding='same', kernel_initializer=tf.keras.initializers.glorot_normal(), \n","                                  name='conv_5x5') (input)\n","\n","  leaky_relu_1 = tf.keras.layers.LeakyReLU(alpha = 0.01, name='leaky_1') (conv_1)                               \n","\n","  maxpool_2 = tf.keras.layers.MaxPooling2D(pool_size=3, strides=3,\n","                                          padding='same',name='maxpool_3x3') (leaky_relu_1)\n","\n","  conv_2 = tf.keras.layers.Conv2D(filters=64,kernel_size=2,strides=(1,1),padding='same',\n","                                  kernel_initializer=tf.keras.initializers.glorot_normal(),name='conv_2x2') (maxpool_2)\n","\n","  leaky_relu_2 = tf.keras.layers.LeakyReLU(alpha = 0.01, name='leaky_2') (conv_2)                               \n","\n","  maxpool_3 = tf.keras.layers.MaxPooling2D(pool_size=3,strides=3,\n","                                          padding='same',name='maxpool_3x3_') (leaky_relu_2)\n","\n","  resize = tf.keras.layers.experimental.preprocessing.Resizing(12,12) (maxpool_3)\n","\n","  dropout_1 = tf.keras.layers.Dropout(0.1) (resize)\n","\n","  # Branch 3\n","  conv_3 = tf.keras.layers.Conv2D(filters=64,kernel_size=5,strides=(1,1),padding='same',\n","                                  kernel_initializer=tf.keras.initializers.glorot_normal(),name='conv_5x5_') (input)\n","\n","  leaky_relu_3 = tf.keras.layers.LeakyReLU(alpha = 0.01, name='leaky_3') (conv_3)                               \n","\n","  maxpool_4 = tf.keras.layers.MaxPooling2D(pool_size=10,strides=10,\n","                                          padding='same',name='maxpool_10x10_') (leaky_relu_3)\n","\n","  dropout_2 = tf.keras.layers.Dropout(0.1) (maxpool_4)\n","\n","  # Merge Branch\n","  merge = tf.keras.layers.concatenate([maxpool_1,dropout_1,dropout_2],axis=3)\n","\n","  conv_4 = tf.keras.layers.Conv2D(filters=64,kernel_size=2,strides=(1,1),padding='same',\n","                                  kernel_initializer=tf.keras.initializers.glorot_normal(),name='conv_2x2_',\n","                                  activation='relu') (merge)\n","\n","  maxpool_5 = tf.keras.layers.MaxPooling2D(pool_size=2,strides=2, padding='same',name='maxpool_2x2_') (conv_4)\n","\n","  flatten = tf.keras.layers.Flatten()(maxpool_5)\n","\n","  fc1 = tf.keras.layers.Dense(units=8,kernel_initializer=tf.keras.initializers.glorot_normal(),\n","                              activation='relu',name='FC1', kernel_regularizer='l2') (flatten)\n","\n","  dropout_3 = tf.keras.layers.Dropout(0.1) (fc1)\n","\n","  output = tf.keras.layers.Dense(units=2,kernel_initializer=tf.keras.initializers.glorot_normal(),\n","                                activation='softmax',name='output') (dropout_3)\n","\n","  # NCNN\n","  ncnn = tf.keras.models.Model(inputs=input,outputs=output)\n","  return ncnn"],"metadata":{"id":"nJUJnVo-osns"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Função de pré-processamento das imagens"],"metadata":{"id":"bg4n9fvLsLRm"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T22:57:34.688052Z","iopub.status.busy":"2022-10-05T22:57:34.687740Z","iopub.status.idle":"2022-10-05T22:57:37.094894Z","shell.execute_reply":"2022-10-05T22:57:37.091712Z","shell.execute_reply.started":"2022-10-05T22:57:34.688024Z"},"id":"KcNj6ZrSBKc-"},"outputs":[],"source":["#pré-processamento para o modelo Inception-V3\n","preprocess_incepV3 = tf.keras.applications.inception_v3.preprocess_input\n","\n","from tensorflow.keras import backend\n","\n","\n","# função de pré-processamento usada no loop de treino\n","def preprocess_input(x, model_name):\n","\n","  # Inception-V3\n","  if model_name==\"incepV3\":\n","\n","    return preprocess_incepV3(x)\n","\n","  # N-CNN\n","  elif model_name == \"ncnn\":\n","\n","    return x / 255\n","\n","  # VGG-16, ResNet50 e SENet50 --> conforme o repositório de R. C. Malli\n","  else:\n","  \n","    # formato da imagem (canal na primeira ou na última dimensão)\n","    data_format = backend.image_data_format()\n","    \n","    if data_format == 'channels_first':\n","      # 'RGB'->'BGR'\n","      if backend.ndim(x) == 3:\n","        x = x[::-1, ...]\n","      else:\n","        x = x[:, ::-1, ...]\n","    else:\n","      # 'RGB'->'BGR'\n","      x = x[..., ::-1]\n","\n","    # valor médio dos canais da imagem, considerando a base do treinamento original\n","    # de cada modelo\n","\n","    if model_name ==\"vgg16\":\n","      mean = [93.5940, 104.7624, 129.1863]  #vggface  --> VGG-16\n","\n","    else:\n","      mean=[91.4953, 103.8827, 131.0912]   #vggface2  --> ResNet50, SENet50\n","\n","    # desvio padrão da base\n","    std = None\n","\n","    # tensor médio\n","    mean_tensor = backend.constant(-np.array(mean))\n","\n","    # Centralização da imagem em zero, utilizando o tensor médio da base\n","    if backend.dtype(x) != backend.dtype(mean_tensor):\n","      x = backend.bias_add(\n","          x, backend.cast(mean_tensor, backend.dtype(x)), data_format=data_format)\n","    else:\n","      x = backend.bias_add(x, mean_tensor, data_format)\n","      \n","    if std is not None:\n","      std_tensor = backend.constant(np.array(std), dtype=backend.dtype(x))\n","      if data_format == 'channels_first':\n","        std_tensor = backend.reshape(std_tensor, (-1, 1, 1))\n","\n","      x /= std_tensor\n","    return x"]},{"cell_type":"markdown","source":["Funções auxiliares"],"metadata":{"id":"jljophyiwmAg"}},{"cell_type":"code","source":["#Criação de Log\n","def log_print(texto):\n","\n","    print(texto)\n","    with open('./log.txt',\"a\") as txt:\n","        txt.write(texto + '\\n')\n"],"metadata":{"id":"hK6gZvlBuP8M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"gradient":{"editing":false,"id":"1834ec44-2093-4d71-8b97-2c66a24a88d8","kernelId":"0e37b7b3-4d80-4a5d-9bda-95cf45add740"},"id":"qwGn1bFUDy93"},"source":["# Carregando imagens"]},{"cell_type":"markdown","source":["O presente trabalho utilizou a união de duas bases de dados distintas.\n","Antes da execução do script a seguir, as imagens de cada base foram organizadas em pastas de acordo com o indivíduo da amostra. Além disso, as imagens originais e as imagens provenientes do processo de Data Augmentation foram organizadas em diretórios distintos\n","\n","Por exemplo:\n","\n","* Imagens Originais\n","  *   Base A\n","    *   ID_01\n","        * ID01_imagem_1\n","        * ID01_imagem_2\n","        * ...\n","        * ID01_imagem_*N*\n","\n","    * ID_02\n","    * ID_03\n","    * ...\n","    * ID_*N*\n","\n","  * Base B\n","\n","* Imagens Data Augmentation\n","  *   Base A\n","    *   ID_01\n","        * ID01_imagemAUG_1\n","        * ID01_imagemAUG_2\n","        * ...\n","        * ID01_imagemAUG_*N*\n","\n","    * ID_02\n","    * ID_03\n","    * ...\n","    * ID_*N*\n","\n","  * Base B"],"metadata":{"id":"H7blK3zsAerA"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-03T23:11:22.852636Z","iopub.status.busy":"2022-10-03T23:11:22.852339Z","iopub.status.idle":"2022-10-03T23:11:58.621261Z","shell.execute_reply":"2022-10-03T23:11:58.619924Z","shell.execute_reply.started":"2022-10-03T23:11:22.852599Z"},"gradient":{"editing":false,"id":"e3ea8e1b-99b1-4e08-9ee5-30f413490de5","kernelId":"0e37b7b3-4d80-4a5d-9bda-95cf45add740"},"id":"fknUP8iBD2BQ"},"outputs":[],"source":["# imagens das bases iCOPE e UNIFESP, separadas por subject\n","!unzip -q ./COPE_UNIFESP_NB.zip \n","\n","# imagens provenientes de Data Augmentation das bases iCOPE e UNIFESP, \n","# separadas por subject\n","!unzip -q ./COPE_UNIFESP_NB_aug.zip\n","\n","# diretórios auxiliares\n","!mkdir ./History\n","!mkdir ./ROC\n","!mkdir ./ConfusionMatrix\n","!mkdir ./Resultados\n","!mkdir ./COPE\n","!mkdir ./UNIFESP\n","!mkdir ./COPE+UNIFESP\n","!mkdir ./UC_weights\n","\n","!mkdir ./COPE+UNIFESP/Fold0\n","!mkdir ./COPE+UNIFESP/Fold1\n","!mkdir ./COPE+UNIFESP/Fold2\n","!mkdir ./COPE+UNIFESP/Fold3\n","!mkdir ./COPE+UNIFESP/Fold4\n","!mkdir ./COPE+UNIFESP/Fold5\n","!mkdir ./COPE+UNIFESP/Fold6\n","!mkdir ./COPE+UNIFESP/Fold7\n","!mkdir ./COPE+UNIFESP/Fold8\n","!mkdir ./COPE+UNIFESP/Fold9\n"]},{"cell_type":"markdown","source":["# Treinamento e Teste"],"metadata":{"id":"87XIRJo1G8i-"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-05T22:58:19.032350Z","iopub.status.busy":"2022-10-05T22:58:19.031983Z","iopub.status.idle":"2022-10-06T01:28:42.318276Z","shell.execute_reply":"2022-10-06T01:28:42.315644Z","shell.execute_reply.started":"2022-10-05T22:58:19.032323Z"},"gradient":{"editing":false,"id":"baa07827-faef-4862-95d6-e71275aeb13b","kernelId":"0e37b7b3-4d80-4a5d-9bda-95cf45add740"},"id":"Z-BeJCi1EB4i"},"outputs":[],"source":["# dict para armazenar os resultados das iterações da validação cruzada\n","COPE_UNIFESP={}\n","\n","# versão do modelo\n","version=\"_v1_\"\n","\n","# qtd de camadas para fine-tuning\n","\n","if model_name =\"vgg16\":\n","  fine_tune_at = 14 #grupos conv4 e conv5 da Vgg-16\n","\n","elif model_name =\"resnet50\":\n","  fine_tune_at = 36  # grupo conv5 da ResNet50\n","\n","elif model_name = \"senet50\":\n","  fine_tune_at = 59 # grupo conv5 da SENet50\n","\n","elif model_name = \"incepv3\":\n","  fine_tune_at = 68 # 18 camadas conv. da Inception-V3\n","\n","else:\n","  fine_tune_at = None  # não haverá fine-tuning\n","\n","# diretórios das imagens originais\n","path_cope = './COPE_UNIFESP_NB/COPE_NB'\n","path_unifesp = './COPE_UNIFESP_NB/UNIFESP_NB'\n","\n","# diretórios das imagens criadas pelo Data Augmentation\n","path_cope_aug = './COPE_UNIFESP_NB_aug/COPE_NB_aug'\n","path_unifesp_aug = './COPE_UNIFESP_NB_aug/UNIFESP_NB_aug'\n","\n","# arquivos .npy com os IDs dos subjects destinados a treinamento e teste em \n","# cada iteração da validação cruzada\n","treino = np.load('./NB_KFold10_Train.npy',allow_pickle=True)\n","teste = np.load('./NB_KFold10_Test.npy',allow_pickle=True)\n","\n","#bancos de imagens\n","DBS = ['COPE+UNIFESP']\n","\n","# loop de treino\n","for DB in DBS:\n","\n","  # 10 repetições por banco de imagens\n","  for contador in range(10):\n","\n","    # Inicio do Log do treinamento\n","    log_print(DB+' Contagem: '+ str(contador))\n","    \n","    # instanciando modelo\n","\n","    if model_name in ['vgg16','incepv3']:\n","\n","      # instanciando o modelo pré-treinado (sem camadas FC)\n","      PRE_TRAINED =  base_model(model_name)    \n","      PRE_TRAINED.trainable = False\n","\n","      # inclusão do novo conjunto de camadas FC no final do modelo\n","      # duas camadas FC com 512 neuronios e uma saída softmax de 2 neurônios\n","      inputs = PRE_TRAINED.input\n","      x = Flatten()(PRE_TRAINED.output)\n","      x = Dense(512,activation='relu',kernel_regularizer=REGULARIZER, dtype='float32')(x)\n","      x = Dropout(0.5)(x) \n","      x = Dense(512,activation='relu',kernel_regularizer=REGULARIZER, dtype='float32')(x)\n","      x = Dropout(0.5)(x)\n","      outputs = Dense(2,activation='softmax', dtype='float32')(x)\n","      # novo modelo\n","      FT_MODEL = tf.keras.Model(inputs,outputs)\n","\n","    elif model_name in ['resnet50','senet50']:\n","\n","      # instanciando o modelo pré-treinado (sem camadas FC)\n","      PRE_TRAINED =  base_model(model_name)    \n","      PRE_TRAINED.trainable = False\n","\n","      # inclusão do novo conjunto de camadas FC no final do modelo\n","      # uma camada FC com 1000 neuronios e uma saída softmax de 2 neurônios\n","      inputs = PRE_TRAINED.input \n","      x = Dense(1000,activation='relu',kernel_regularizer=REGULARIZER, dtype='float32')(PRE_TRAINED.output)\n","      x = Dropout(0.5)(x)\n","      outputs = Dense(2,activation='softmax', dtype='float32')(x)\n","      # novo modelo\n","      FT_MODEL = tf.keras.Model(inputs,outputs)\n","\n","    else:\n","      FT_MODEL = NCNN()\n","\n","    \n","\n","    #criaçãos dos diretórios das imagens\n","    if os.path.isdir('./dataset'):\n","      !rm -r ./dataset\n","      \n","    !mkdir ./dataset\n","\n","    !mkdir ./dataset/train\n","\n","    !mkdir ./dataset/train/COPE+UNIFESP\n","    !mkdir ./dataset/train/COPE+UNIFESP/dor\n","    !mkdir ./dataset/train/COPE+UNIFESP/sem_dor\n","\n","    !mkdir ./dataset/validation\n","    !mkdir ./dataset/validation/dor\n","    !mkdir ./dataset/validation/sem_dor\n","\n","    # separaçãos da imagens de treinamento e teste\n","    x_train, x_test = treino[contador].tolist(), teste[contador].tolist()    \n","\n","    # abastecimento diretórios de imagens de treinamento\n","    for i in range(len(x_train)):\n","\n","      # para cada subject de treinamento em x_train\n","      # avalia-se sua base de origem é UNIFESP (U) ou iCOPE (C)\n","      base = x_train[i][0]  # U ou C\n","\n","      # conforme a base, são determinados os diretórios para coleta das imagens\n","      # originais e das imagens provenientes do Data Augmentation  \n","      path = path_cope if base == 'C' else path_unifesp\n","      path_aug = path_cope_aug if base == 'C' else path_unifesp_aug      \n","      \n","\n","      # Movimentação das imagens do subject para o diretório de imagens de treinamento    \n","      src = osp.join(path,x_train[i])\n","      src_aug = osp.join(path_aug,x_train[i]+\"_aug\")\n","\n","      for img in os.listdir(osp.join(src,\"Dor\")):\n","        copyfile(osp.join(src,\"Dor\",img),osp.join('./dataset/train/COPE+UNIFESP/dor',img))\n","\n","      for img in os.listdir(osp.join(src_aug,\"Dor\")):\n","        copyfile(osp.join(src_aug,\"Dor\",img),osp.join('./dataset/train/COPE+UNIFESP/dor',img))\n","\n","      for img in os.listdir(osp.join(src,\"Sem_dor\")):\n","        copyfile(osp.join(src,\"Sem_dor\",img),osp.join('./dataset/train/COPE+UNIFESP/sem_dor',img))\n","\n","      for img in os.listdir(osp.join(src_aug,\"Sem_dor\")):\n","        copyfile(osp.join(src_aug,\"Sem_dor\",img),osp.join('./dataset/train/COPE+UNIFESP/sem_dor',img))\n","\n","\n","    # abastecimento diretórios de imagens de teste\n","    for i in range(len(x_test)):\n","\n","      # para cada subject de teste em x_test\n","      # avalia-se sua base de origem é UNIFESP (U) ou iCOPE (C)\n","      base = x_test[i][0]  # U ou C\n","\n","      # conforme a base, são determinados os diretórios para coleta das imagens\n","      # originais e das imagens provenientes do Data Augmentation  \n","      path = path_cope if base == 'C' else path_unifesp\n","      src = osp.join(path,x_test[i])\n","\n","      # Movimentação das imagens do subject para o diretório de imagens de teste  \n","      for img in os.listdir(osp.join(src,\"Dor\")):\n","        copyfile(osp.join(src,\"Dor\",img),osp.join('./dataset/validation/dor',img))\n","\n","      for img in os.listdir(osp.join(src,\"Sem_dor\")):\n","        copyfile(osp.join(src,\"Sem_dor\",img),osp.join('./dataset/validation/sem_dor',img))\n","            \n","\n","    #criação dos datasets (objeto TensorFlow) de treinamento e teste\n","    PATH = './dataset/'\n","\n","    train_ds = tf.keras.preprocessing.image_dataset_from_directory(PATH+'train/'+DB,\n","                                                                  image_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                                  batch_size=BATCH_SIZE,\n","                                                                  label_mode='binary')\n","\n","    val_ds = tf.keras.preprocessing.image_dataset_from_directory(PATH+'validation',\n","                                                                image_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                                batch_size=BATCH_SIZE,\n","                                                                label_mode='binary')\n","\n","    #aplicação da função de pré-processamento nas imagens dos datasets\n","    train_ds = train_ds.map(lambda x, y: (preprocess_input(x, model_name), y), num_parallel_calls=AUTOTUNE)\n","    val_ds = val_ds.map(lambda x, y: (preprocess_input(x, model_name), y), num_parallel_calls=AUTOTUNE)\n","\n","    train_ds = train_ds.cache().shuffle(BUFFER_SIZE).prefetch(AUTOTUNE)\n","    val_ds = val_ds.cache().prefetch(AUTOTUNE)\n","\n","    # Diretório para gravação do modelo originado pela iteração\n","    fold = './'+DB+'/Fold' + str(contador) + \"/\"\n","\n","    # Callback para gravação do modelo de menor erro durante o treinamento. Apenas os pesos são salvos.\n","    check = ModelCheckpoint(fold + model_name + version + DB + \"_\" + str(contador) + '_weights.h5',\n","                        monitor='val_loss',\n","                        verbose=0,\n","                        save_best_only=True,\n","                        save_weights_only=True)\n","\n","    # Critério de parada do treinamento: \n","    # - 5 épocas consecutivas sem redução do erro p/ modelos pré-treinados\n","    # - 10 épocas consecutivas sem redução do erro p/ N-CNN\n","\n","    patience = 5 if fine_tune_at else 10\n","    stop = EarlyStopping(monitor='val_loss',\n","                         patience=patience,\n","                         verbose=0)\n","\n","    # compilando o modelo...\n","    FT_MODEL.compile(optimizer=OPTIMIZER,\n","                 loss=LOSS,\n","                 metrics='acc')\n","    \n","    # Treinamento da CNN\n","    history = FT_MODEL.fit(train_ds,\n","                          validation_data=val_ds,\n","                          epochs=EPOCHS,\n","                          callbacks=[check, stop],\n","                          verbose=0\n","                          )\n","    \n","    ft=False  ## variável auxiliar que será utilizada na finalização do log (indica se houve fine-tuning ou não)\n","\n","    ## FINE TUNING\n","    # se o modelo for  pré-treinado e seu novo treinamento foi encerrado pelo critério das 5 épocas consecutivas, \n","    # isto é,o treinamento não alcançou o total de épocas, inicia-se o fine-tuning\n","    # *OBS: não há fine-tuning para a N-CNN!\n","    if (len(history.history['val_loss']) < EPOCHS) and fine_tune_at:\n","      print(\"Fine Tuning!\")\n","\n","      # Inicia-se o fine-tuning\n","      ft=True\n","      \n","      # Carregando o modelo de melhor acurácia obitido na etapa anterior\n","      FT_MODEL.load_weights(fold + model_name + version + DB + \"_\" + str(contador) + '_weights.h5')\n","\n","      # Todas as camadas são liberadas para treinamento\n","      FT_MODEL.trainable = True\n","\n","      # As camadas anteriores a camada `fine_tune_at` são congeladas, ou seja, \n","      # não sofrerão o ajuste de pesos\n","      for layer in FT_MODEL.layers[:-fine_tune_at]:\n","        layer.trainable =  False\n","\n","      # A partir da camada 'fine_tune_at' até a saída do modelo, todas as camadas\n","      # sofrerão o ajuste de pesos, com exceção das camadas de BatchNormalization\n","      # (caso o modelo tenha), seguindo recomendação do TensorFlow\n","      for layer in FT_MODEL.layers[-fine_tune_at:]:\n","        if isinstance(layer, layers.BatchNormalization):\n","          layer.trainable = False\n","\n","      # Critério de parada do treinamento durante o fine-tuning: 10 épocas consecutivas sem redução do erro\n","      stop_ft = EarlyStopping(monitor='val_loss',\n","                              patience=10,\n","                              verbose=0)\n","      \n","      # Callback para gravação do modelo de menor erro durante o treinamento. Apenas os pesos são salvos.\n","      check_ft = ModelCheckpoint(fold + 'FT_' + model_name + version + DB+\"_\" + str(contador) + '_weights.h5',\n","                        monitor='val_loss',\n","                        verbose=0,\n","                        save_best_only=True,\n","                        save_weights_only=True)\n","\n","      # compilando o modelo...\n","      FT_MODEL.compile(optimizer=OPTIMIZER_ft,\n","                 loss=LOSS,\n","                 metrics='acc')\n","      \n","      # qtd de épocas de fine-tuning.\n","      # selecionou-se um número elevado pois o objetivo é que o treinamento\n","      # seja encerrado pelo critério das 10 épocas\n","      fine_tune_epochs = 100\n","\n","      # ajuste do índice da época para registro do histórico de treinamento.\n","      # Este ajuste evita a sobrescrição no histórico do treinamento anterior ao fine-tuning.\n","      total_epochs =  EPOCHS + fine_tune_epochs\n","\n","      # novo treinamento do modelo\n","      history_fine = FT_MODEL.fit(train_ds,\n","                           validation_data=val_ds,\n","                           epochs=total_epochs,\n","                           initial_epoch=history.epoch[-1],\n","                           callbacks=[check_ft, stop_ft],\n","                           verbose=0\n","                          )\n","\n","\n","    #----------------------------##\n","    ## Testando a CNN ##\n","\n","    # coletando as classes das imagens\n","    y_true = []\n","    for element in val_ds.unbatch():\n","        y_true.append(np.array(element[1]))\n","    \n","    # carregando a melhor versão do modelo na iteração da validação cruzada\n","    try:\n","      # tenta carregar umaa versão proveniente do fine-tuning\n","      FT_MODEL.load_weights(fold +'FT_' + model_name + version + DB+\"_\" + str(contador) + '_weights.h5')\n","\n","    except:\n","      # se não houver, é utilizada uma versão sem fine-tuning\n","      FT_MODEL.load_weights(fold + model_name + version + DB + \"_\" + str(contador) + '_weights.h5')\n","\n","    # a CNN classifica as imagens reservadas para teste\n","    y_pred = FT_MODEL.predict(val_ds)\n","\n","    # tratamento da classificação softmax\n","    y_pred = [0 if np.argmax(y) == 0 else 1 for y in y_pred]\n","\n","    # metricas de desempenho\n","    acc = accuracy_score(y_true,y_pred)\n","    pr = precision_score(y_true,y_pred)\n","    rc = recall_score(y_true,y_pred)\n","    f1 = f1_score(y_true,y_pred)\n","    AUC = roc_auc_score(y_true,y_pred)\n","\n","    # curva ROC\n","    fpr, tpr, th = roc_curve(y_true,y_pred)\n","    roc_dict={'fpr':fpr, 'tpr':tpr, 'th': th}\n","    roc_csv = \"./ROC/ROC_\"+ model_name + version + DB + \"_\" + str(contador)+\".csv\"\n","    pd.DataFrame.from_dict(roc_dict).to_csv(roc_csv,index=False,sep=\";\",decimal=',')\n","\n","    # matriz de confusao\n","    CMatrix_file = './ConfusionMatrix/CM_'+ model_name + version + DB + \"_\" + str(contador)+\".npy\"\n","    cmatrix = confusion_matrix(y_true,y_pred)\n","    np.save(CMatrix_file,cmatrix)\n","\n","    # quantidade de épocas\n","    epochs_hist = len(history.history['val_loss'])\n","\n","    # quantidade de épocas em fine-tuning (se houver)\n","    if ft:\n","      epochs_ft = len(history_fine.history['val_loss']) \n","    else:\n","      epochs_ft= 0\n","    \n","    # registro em log das métricas de desempenho e quantidade de épocas\n","    log_print(f\"Acuracia: {acc:.4f}\\nPrecision: {pr:.4f}\\nRecall: {rc:.4f}\\nF1: {f1:.4f}\\nAUC: {AUC:.4f}\\nEpocas: {epochs_hist}\\nEpocas_FT: {epochs_ft}\")\n","\n","    \n","    # histórico de treinamento\n","    csv_name = \"./History/history_\" + model_name+version+DB+\"_\"+str(contador)+\".csv\"\n","  \n","    if ft:\n","      pd.concat([pd.DataFrame.from_dict(history.history),pd.DataFrame.from_dict(history_fine.history)]).to_csv(csv_name,index=False,sep=\";\",decimal=',')\n","    else:\n","      pd.DataFrame.from_dict(history.history).to_csv(csv_name,index=False,sep=\";\",decimal=',')\n","\n","# fim do loop de treinamento e validação\n","print(\"Fim!!!\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}