{"cells":[{"cell_type":"markdown","source":["# Introdução"],"metadata":{"id":"fIkH0bl43nSS"}},{"cell_type":"markdown","source":["Este notebook apresenta a aplicação do método Gradient-weighted Class Activation Mapping (Grad-CAM) para interpretação das classificações realizadas por 5 CNNs distintas.\n","\n","O Grad-CAM foi proposto por Selvaraju et al (2016) em:\n","\"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\" (disponível em https://arxiv.org/abs/1610.02391)\n","\n","O presente trabalho utilizou a implementação do Grad-CAM apresentada em um exemplo da biblioteca Keras: https://keras.io/examples/vision/grad_cam/"],"metadata":{"id":"rycV9zr73p0W"}},{"cell_type":"markdown","source":["# Configuração"],"metadata":{"id":"LBHSOMDiYRad"}},{"cell_type":"markdown","source":["Bibliotecas"],"metadata":{"id":"l670aJMbYXOl"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T23:54:33.619391Z","iopub.status.busy":"2022-12-14T23:54:33.618463Z","iopub.status.idle":"2022-12-14T23:54:40.338159Z","shell.execute_reply":"2022-12-14T23:54:40.336972Z","shell.execute_reply.started":"2022-12-14T23:54:33.619352Z"},"id":"J9BJs76gRGVB"},"outputs":[],"source":["import os\n","import os.path as osp\n","import numpy as np\n","import pandas as pd\n","from shutil import copyfile\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","source":["Funções auxiliares"],"metadata":{"id":"WkyOr2bEYZc3"}},{"cell_type":"code","source":["# registro em log\n","def log(texto,name):\n","\n","  with open('./'+name+'.txt',\"a\") as txt:\n","      txt.write(texto + '\\n')"],"metadata":{"id":"9mQRg98tYbu9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Funções de pré-processamento das imagens"],"metadata":{"id":"2qTJX-f6Yn01"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-15T00:16:21.247663Z","iopub.status.busy":"2022-12-15T00:16:21.246743Z","iopub.status.idle":"2022-12-15T00:16:21.257350Z","shell.execute_reply":"2022-12-15T00:16:21.255378Z","shell.execute_reply.started":"2022-12-15T00:16:21.247637Z"},"id":"_j32MXsC1Dxx"},"outputs":[],"source":["#VGG\n","def vgg_preprocess_input(x_temp):\n","    x_temp = x_temp[..., ::-1]\n","    x_temp[...,0] -= 93.5940\n","    x_temp[...,1] -= 104.7624\n","    x_temp[...,2] -= 129.1863\n","    \n","    return x_temp\n","\n","#ResNet e SENet\n","def resnet_preprocess_input(x_temp):\n","    x_temp = x_temp[..., ::-1]\n","    x_temp[...,0] -= 91.4953\n","    x_temp[...,1] -= 103.8827\n","    x_temp[...,2] -= 131.0912\n","    \n","    return x_temp\n","\n","#Inception-V3\n","incep_preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n","\n","\n","#N-CNN\n","def ncnn_preprocess_input(x_temp):\n","    \n","    return x_temp/255\n","\n","\n","# Esta função seleciona o pré-processamento em função do modelo\n","def preprocess_input(x_temp, modelo):\n","\n","  if modelo == 'VGG16':\n","    x_temp = vgg_preprocess_input(x_temp)\n","\n","  elif modelo == 'ResNet50' or modelo == 'SENet50':\n","    x_temp = resnet_preprocess_input(x_temp)\n","\n","  elif modelo == 'IncepV3':\n","    x_temp = incep_preprocess_input(x_temp)\n","\n","  else:\n","    x_temp = ncnn_preprocess_input(x_temp)\n","\n","  return x_temp\n"]},{"cell_type":"markdown","source":["### Funções do Grad-CAM"],"metadata":{"id":"rGl-k9o9ZTnV"}},{"cell_type":"markdown","source":["Estas funções foram retiradas de um exemplo da biblioteca Keras, disponível em:\n","https://keras.io/examples/vision/grad_cam/"],"metadata":{"id":"h47aeEFSZwWR"}},{"cell_type":"code","source":["from tensorflow import keras\n","from IPython.display import Image, display\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","\n","# Esta função carrega uma imagem e a converte num arrat\n","def get_img_array(img_path, size):\n","    # `img` é uma PIL image de tamanho HxW\n","    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n","    # `array` é float32 Numpy array de tamanho (H, W, 3)\n","    array = keras.preprocessing.image.img_to_array(img)\n","    # Adiciona-se uma dimensão ao array para transformá-lo num \"batch\"\n","    # de dimensão (1, H, W, 3)\n","    array = np.expand_dims(array, axis=0)\n","    return array\n","\n","\n","# Esta função produz o Mapa de Ativação de Classe do Grad-CAM\n","def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n","\n","    # Inicialmente, é criada uma versão do modelo que mapeia a imagem de entrada\n","    # tanto na ativação da última cada convolucional e quanto na camada de saída.\n","    grad_model = tf.keras.models.Model(\n","        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n","    )\n","\n","    # Em seguida, computa-se o gradiente da classe prevista para imagem de entrada\n","    # com relação a ativação da última camada convolucional\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = grad_model(img_array)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        # canal de saída corresponde a classe determinada\n","        class_channel = preds[:, pred_index]  \n","\n","    # Este é o gradiente do neurônio da saída em relação ao feature map gerado \n","    # pela última camada convolucional\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","    \n","    # Neste vetor, cada valor corresponde a média da intensidade do gradiente \n","    # com relação a um canal específico do feature map.\n","    # Em outras palavras, é a operação de Global Average Pooling aplicada ao \n","    # gradiente para produzir o \"vetor de importâncias\" dos canais\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # Cada canal do feature map é multiplicado pelo valor correspondente no \n","    # \"vetor de importâncias\". Em seguida, é feita a soma de todos os canais \n","    # para a produção do mapa de ativação   \n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    # O mapa é normalizado entre 0 e 1 para possibilitar a visualização\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","    return heatmap.numpy()\n","\n","\n","# Esta função sobrepõe o Mapa de Ativação de Classe na imagem original\n","# e salva o resultado  \n","def save_gradcam(img_path, heatmap, img_size, cam_path=\"cam.jpg\", alpha=0.4):\n","    # Carregando a imagem original\n","    img = keras.preprocessing.image.load_img(img_path, target_size=img_size)\n","    img = keras.preprocessing.image.img_to_array(img)\n","\n","    # Ajuste do mapa para uma escala de 0-255\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # Carregando o colormap 'jet'\n","    jet = cm.get_cmap(\"jet\")\n","\n","    # Aplicando o colormap no mapa\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","\n","    # Criando uma imagem RGB do mapa de ativacao colorido\n","    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n","    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n","    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n","\n","    # Sobreposicao do mapa de ativacao na imagem original\n","    superimposed_img = jet_heatmap * alpha + img\n","    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n","\n","    # Salvando o resultado da sobreposicao    \n","    superimposed_img.save(cam_path)\n","\n","    # Apresentar o resultado da sobreposição na tela\n","    #display(Image(superimposed_img))\n","\n","\n"],"metadata":{"id":"XdU-fpYXZWeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T23:55:28.541351Z","iopub.status.busy":"2022-12-14T23:55:28.539417Z","iopub.status.idle":"2022-12-14T23:55:48.401888Z","shell.execute_reply":"2022-12-14T23:55:48.400648Z","shell.execute_reply.started":"2022-12-14T23:55:28.541293Z"},"id":"4_s2oDPCTClk"},"outputs":[],"source":["!unzip -q ./COPE_UNIFESP_NBfolds.zip \n","base_path = './COPE_UNIFESP_NBfolds'\n","\n","!mkdir ./GradCAM_NBfold4"]},{"cell_type":"markdown","source":["# Aplicação do Grad-CAM nos modelos"],"metadata":{"id":"HzLYgT7ziO8G"}},{"cell_type":"markdown","source":["### Criação dos diretórios para salvar os resultados"],"metadata":{"id":"CiqoaFFikRet"}},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-06T19:49:44.982610Z","iopub.status.busy":"2022-11-06T19:49:44.982209Z","iopub.status.idle":"2022-11-06T19:49:51.036241Z","shell.execute_reply":"2022-11-06T19:49:51.034885Z","shell.execute_reply.started":"2022-11-06T19:49:44.982610Z"},"id":"FriBZ76A3gWi"},"outputs":[],"source":["# VGG\n","\n","# Resultados gerais\n","!mkdir ./VGG16_GradCAM\n","!mkdir ./VGG16_GradCAM/All\n","!mkdir ./VGG16_GradCAM/All/Right\n","!mkdir ./VGG16_GradCAM/All/Wrong\n","\n","# Resultados separados por Fold da validação cruzada\n","for i in range(10):\n","\n","  os.mkdir('./VGG16_GradCAM/Fold'+str(i))\n","  os.mkdir('./VGG16_GradCAM/Fold'+str(i)+'/Right')\n","  os.mkdir('./VGG16_GradCAM/Fold'+str(i)+'/Wrong')\n"]},{"cell_type":"code","source":["# ResNet50\n","\n","# Resultados gerais\n","!mkdir ./ResNet50_GradCAM\n","!mkdir ./ResNet50_GradCAM/All\n","!mkdir ./ResNet50_GradCAM/All/Right\n","!mkdir ./ResNet50_GradCAM/All/Wrong\n","\n","# Resultados separados por Fold da validação cruzada\n","for i in range(10):\n","\n","  os.mkdir('./ResNet50_GradCAM/Fold'+str(i))\n","  os.mkdir('./ResNet50_GradCAM/Fold'+str(i)+'/Right')\n","  os.mkdir('./ResNet50_GradCAM/Fold'+str(i)+'/Wrong')\n"],"metadata":{"id":"ONj-dH4ikYxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SENet50\n","\n","# Resultados gerais\n","!mkdir ./SENet50_GradCAM\n","!mkdir ./SENet50_GradCAM/All\n","!mkdir ./SENet50_GradCAM/All/Right\n","!mkdir ./SENet50_GradCAM/All/Wrong\n","\n","# Resultados separados por Fold da validação cruzada\n","for i in range(10):\n","\n","  os.mkdir('./SENet50_GradCAM/Fold'+str(i))\n","  os.mkdir('./SENet50_GradCAM/Fold'+str(i)+'/Right')\n","  os.mkdir('./SENet50_GradCAM/Fold'+str(i)+'/Wrong')\n"],"metadata":{"id":"ZlmD8NcPke1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inception-V3\n","\n","# Resultados gerais\n","!mkdir ./IncepV3_GradCAM\n","!mkdir ./IncepV3_GradCAM/All\n","!mkdir ./IncepV3_GradCAM/All/Right\n","!mkdir ./IncepV3_GradCAM/All/Wrong\n","\n","# Resultados separados por Fold da validação cruzada\n","for i in range(10):\n","\n","  os.mkdir('./IncepV3_GradCAM/Fold'+str(i))\n","  os.mkdir('./IncepV3_GradCAM/Fold'+str(i)+'/Right')\n","  os.mkdir('./IncepV3_GradCAM/Fold'+str(i)+'/Wrong')\n"],"metadata":{"id":"wNp2n3WNklGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# N-CNN\n","\n","# Resultados gerais\n","!mkdir ./NCNN_GradCAM\n","!mkdir ./NCNN_GradCAM/All\n","!mkdir ./NCNN_GradCAM/All/Right\n","!mkdir ./NCNN_GradCAM/All/Wrong\n","\n","# Resultados separados por Fold da validação cruzada\n","for i in range(10):\n","\n","  os.mkdir('./NCNN_GradCAM/Fold'+str(i))\n","  os.mkdir('./NCNN_GradCAM/Fold'+str(i)+'/Right')\n","  os.mkdir('./NCNN_GradCAM/Fold'+str(i)+'/Wrong')\n"],"metadata":{"id":"J1r_bHoDktTe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##  Grad-CAM"],"metadata":{"id":"w_JBDdQim1hN"}},{"cell_type":"markdown","source":["Inicialmente, é preciso determinar qual modelo será submetido ao Grad-CAM.\n","Opções: VGG16, ResNet50, SENet50, Inception-V3 e N-CNN"],"metadata":{"id":"4SQqnitRnQeu"}},{"cell_type":"code","source":["modelo = \"VGG16\"\n","#modelo = \"ResNet50\"\n","#modelo = \"SENet50\"\n","#modelo = \"IncepV3\"\n","#modelo = \"NCNN\""],"metadata":{"id":"cTSBfbmcm9jw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para a execução deste script, a base de dados (especificamente, a união das bases utilizadas) foi organizada em pastas que correspondem aos *folds* da validação cruzada, conforme o exemplo a seguir:\n","\n","*   Base A\n","  *   fold_01\n","      * Imagem_1\n","      * Imagem_2\n","      * ...\n","      * Imagem_*N*\n","\n","  * fold_02\n","  * fold_03\n","  * ...\n","  * fold_*N*"],"metadata":{"id":"ZlPHeDw_nubq"}},{"cell_type":"code","source":["# enderço da base de dados divida em folds\n","\n","base_path = './COPE_UNIFESP_NBfolds'"],"metadata":{"id":"eDyXgUijnyaW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Estruturas auxiliares"],"metadata":{"id":"L19t4-ghnhit"}},{"cell_type":"code","source":["# dicts utilizados para ajustar a rotina do gradcam para cada modelo\n","\n","# endereços para gravação dos resultados\n","path_dict={\"VGG16\": './VGG16_GradCAM',\n","           \"ResNet50\": './ResNet50_GradCAM',\n","           \"SENet50\": './SENet50_GradCAM',\n","           \"IncepV3\": './IncepV3_GradCAM',\n","           \"NCNN\": './NCNN_GradCAM'}\n","\n","# dimensões das imagens\n","size_dict={\"VGG16\": (224,224),\n","           \"ResNet50\": (224,224),\n","           \"SENet50\": (224,224),\n","           \"IncepV3\": (299,299),\n","           \"NCNN\": (120,120)}\n","\n","# Nome da camada analisada em cada modelo\n","# Trata-se da camada imediatamente anterior a última cada\n","# de pooling do modelo.\n","# Este parâmetro muda conforme o arquivo .h5 utilizado, \n","# portanto, talvez seja necessário analisar os nomes das camadas\n","# da CNN antes de prosseguir.\n","conv_layer_dict = {\"VGG16\": \"conv5_3\",\n","                  \"ResNet50\": \"activation_48\",\n","                  \"SENet50\": \"activation_161\",\n","                  \"IncepV3\": \"mixed10\",\n","                  \"NCNN\": \"conv_2x2_\"}\n","\n","# nome do arquivo .h5 do modelo\n","model_dict = {\"VGG16\": \"VGG16.h5\",\n","              \"ResNet50\": \"ResNet50.h5\",\n","              \"SENet50\": \"SENet50.h5\",\n","              \"IncepV3\": \"InceptionV3.h5\",\n","              \"NCNN\": \"NCNN.h5\"}"],"metadata":{"id":"EtMWRnkhjPly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Rotina de aplicação do Grad-CAM e gravação dos resultados"],"metadata":{"id":"RZqc8hm3nk3E"}},{"cell_type":"code","source":["# endereço para gravação dos resultados\n","file_path= path_dict[modelo]\n","\n","# dimensões das imagens para o modelo escolhido\n","img_size= size_dict[modelo]\n","\n","# nome da camada para aplicação do Grad-CAM\n","last_conv_layer_name = conv_layer_dict[modelo]\n","\n","# nome do arquivo .h5 do modelo\n","model_file = model_dict[modelo]\n","\n","# O modelo é instaciado duas vezes\n","# Na primeira, o modelo é carregado normalmente\n","rede = load_model(model_file)\n","\n","# Na segunda, a ativação da camada de saída é desabilitada\n","model = load_model(model_file)\n","# Remoção da função softmax do modelo\n","model.layers[-1].activation = None\n","\n","# lista de resultados\n","rlist=[]\n","\n","# varrendo folds da base de dados\n","for fold in os.listdir(base_path):\n","\n","  # endereço do fold\n","  fold_path = osp.join(base_path,fold)\n","\n","  # varrendo imagens do fold\n","  for img_name in os.listdir(fold_path):\n","    \n","    # endereço da imagem\n","    img_path = osp.join(fold_path,img_name)\n","\n","    # verdadeira classe da imagem\n","    real_label=''\n","\n","    # imagens \"Sem Dor\" da base iCOPE possuem a palavra 'rest' no nome\n","    # imagens \"Sem Dor\" da base UNIFESP possuem o termo 'sem_dor' no nome\n","    if ('sem' in img_name) or ('rest' in img_name):\n","      real_label = 'No_pain'\n","    else:\n","      real_label = 'Pain'\n","    \n","    # lista de possíveis classes da previsão do modelo\n","    lab_preds=['Pain','No_pain']\n","    \n","    # conversao da imagem em array e pré-processamento\n","    img_array = preprocess_input(get_img_array(img_path, size=img_size), modelo)\n","\n","    # classificacao pelo modelo original, isto é, a versão que manteve a ativacao\n","    # da saída\n","    preds = rede.predict(img_array)\n","\n","    # indíce da classe de maior probabilidade na saída softmax\n","    index = np.argmax(preds)\n","    \n","    # determina se o modelo acertou a classificação, ou seja, se a classe prevista\n","    # é igual a classe verdadeira\n","    correto = 'Right' if real_label == lab_preds[index] else 'Wrong'\n","    \n","    # a confiabilidade do modelo na classificação\n","    conf = str(round(preds[0][index]*100,2)) +\"%\"\n","\n","    # um linha do relatório que terá os resultados de cada classificação\n","    resultado = (fold,img_name.replace('.png',''),real_label,lab_preds[index],conf,correto)\n","    rlist.append(resultado)\n","    \n","    # aplicação do GradCAM para obtenção do mapa de ativação de classe\n","    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=index)\n","    \n","    # salvando o mapa de ativação da pasta geral\n","    cam_path_all=osp.join(file_path,'All',correto,modelo + \"_\"+img_name)\n","    save_gradcam(img_path, heatmap, img_size, cam_path_all, alpha=0.4)\n","\n","    # salvando o mapa de ativação da pasta exclusiva do fold em questão\n","    cam_path_fold=osp.join(file_path,fold,correto,modelo + \"_\"+img_name)\n","    save_gradcam(img_path, heatmap, img_size, cam_path_fold, alpha=0.4)\n","\n","# criação do relatório (planilha) com os resultados de cada classificacao\n","columns = [\"Fold\",\"Image\",\"Class\",\"Pred\",\"Conf\",\"Right?\"]\n","df = pd.DataFrame(data = rlist,columns = columns)\n","csv_name = osp.join(file_path,modelo + \".csv\")\n","df.to_csv(csv_name,index=False,sep=\";\",decimal=',')"],"metadata":{"id":"1pyhnt-elCtL"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}